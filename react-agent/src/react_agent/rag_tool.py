# RAG (Retrieval-Augmented Generation) ë„êµ¬
# Chroma DBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ê²€ìƒ‰ ë° ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥

import os
import logging
from pathlib import Path
from typing import Optional, List, Dict, Any
from react_agent.cache_manager import get_cache_manager

try:
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document
    from rank_bm25 import BM25Okapi
    import numpy as np
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

logger = logging.getLogger(__name__)


class RAGTool:
    """RAG ê²€ìƒ‰ ë„êµ¬ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        knowledge_base_path: Optional[str] = None,
        chroma_db_path: Optional[str] = None
    ):
        """
        RAG ë„êµ¬ ì´ˆê¸°í™”

        Args:
            knowledge_base_path: ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ê²½ë¡œ
            chroma_db_path: Chroma DB ì €ì¥ ê²½ë¡œ
        """
        if not RAG_AVAILABLE:
            logger.warning("RAG ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.available = False
            return

        self.available = True
        self._kb_last_modified: Optional[float] = None  # ì§€ì‹ë² ì´ìŠ¤ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„
        
        # ê²½ë¡œ ì„¤ì •
        if knowledge_base_path is None:
            knowledge_base_path = os.getenv(
                "KNOWLEDGE_BASE_PATH",
                str(Path(__file__).parent.parent.parent / "knowledge_base")
            )
        if chroma_db_path is None:
            chroma_db_path = os.getenv(
                "CHROMA_DB_PATH",
                str(Path(__file__).parent.parent.parent / "chroma_db")
            )
        
        self.knowledge_base_path = Path(knowledge_base_path)
        self.chroma_db_path = Path(chroma_db_path)
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': 'cpu'}
            )
            logger.info("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° (ì˜ë¯¸ì  ì²­í‚¹ ì „ëµ)
        # í•œêµ­ì–´ ë¬¸ì„œì— ìµœì í™”ëœ separator ìš°ì„ ìˆœìœ„:
        # 1. ë¹ˆ ì¤„ 3ê°œ (ì„¹ì…˜ êµ¬ë¶„)
        # 2. ë¹ˆ ì¤„ 2ê°œ (ë¬¸ë‹¨ êµ¬ë¶„)
        # 3. ë¹ˆ ì¤„ 1ê°œ (ì†Œë¬¸ë‹¨ êµ¬ë¶„)
        # 4. í•œêµ­ì–´/ì˜ì–´ ë§ˆì¹¨í‘œ (ë¬¸ì¥ ë‹¨ìœ„)
        # 5. í•œêµ­ì–´ ì‰¼í‘œ, ê³µë°± (ì–´ì ˆ ë‹¨ìœ„)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # ë” ì‘ì€ ì²­í¬ë¡œ ì§‘ì¤‘ëœ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
            chunk_overlap=150,  # 20% ì˜¤ë²„ë©ìœ¼ë¡œ ë¬¸ë§¥ ì—°ì†ì„± ìœ ì§€
            length_function=len,
            separators=[
                "\n\n\n",    # ì„¹ì…˜ êµ¬ë¶„ (ìµœìš°ì„ )
                "\n\n",      # ë¬¸ë‹¨ êµ¬ë¶„
                "\n",        # ì¤„ë°”ê¿ˆ
                ". ",        # ì˜ì–´ ë§ˆì¹¨í‘œ
                "ã€‚",        # í•œêµ­ì–´/ì¤‘êµ­ì–´ ë§ˆì¹¨í‘œ
                "! ",        # ëŠë‚Œí‘œ
                "? ",        # ë¬¼ìŒí‘œ
                ".",         # ë§ˆì¹¨í‘œë§Œ
                ", ",        # ì‰¼í‘œ
                "ï¼Œ",        # í•œêµ­ì–´/ì¤‘êµ­ì–´ ì‰¼í‘œ
                " ",         # ê³µë°± (ìµœí›„)
                ""           # ë¬¸ì ë‹¨ìœ„ (ìµœí›„ì˜ ìˆ˜ë‹¨)
            ],
            keep_separator=True  # separatorë¥¼ ìœ ì§€í•˜ì—¬ ë¬¸ë§¥ ë³´ì¡´
        )
        
        # ë²¡í„° ìŠ¤í† ì–´ (ì§€ì—° ë¡œë”©)
        self._vectorstore: Optional[Chroma] = None

        # BM25 ì¸ë±ìŠ¤ (ì§€ì—° ë¡œë”©)
        self._bm25_index: Optional['BM25Okapi'] = None
        self._bm25_documents: List[Document] = []  # BM25ìš© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        # ì§€ì‹ë² ì´ìŠ¤ ë³€ê²½ ê°ì§€ ì´ˆê¸°í™”
        self._update_kb_modified_time()

        logger.info(f"RAG ë„êµ¬ ì´ˆê¸°í™” ì™„ë£Œ: {knowledge_base_path}")

    def _get_kb_modified_time(self) -> Optional[float]:
        """ì§€ì‹ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ì˜ ìµœì‹  ìˆ˜ì • ì‹œê°„ ë°˜í™˜"""
        if not self.knowledge_base_path.exists():
            return None

        try:
            latest_time = 0.0
            for ext in ['.txt', '.md', '.pdf', '.docx']:
                for file_path in self.knowledge_base_path.rglob(f"*{ext}"):
                    mtime = file_path.stat().st_mtime
                    if mtime > latest_time:
                        latest_time = mtime
            return latest_time if latest_time > 0 else None
        except Exception as e:
            logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ìˆ˜ì • ì‹œê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None

    def _update_kb_modified_time(self):
        """ì§€ì‹ë² ì´ìŠ¤ ìˆ˜ì • ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self._kb_last_modified = self._get_kb_modified_time()
        if self._kb_last_modified:
            logger.info(f"ì§€ì‹ë² ì´ìŠ¤ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„: {self._kb_last_modified}")

    def _check_kb_changed(self) -> bool:
        """ì§€ì‹ë² ì´ìŠ¤ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        current_time = self._get_kb_modified_time()
        if current_time is None or self._kb_last_modified is None:
            return False

        changed = current_time > self._kb_last_modified
        if changed:
            logger.info("ì§€ì‹ë² ì´ìŠ¤ ë³€ê²½ ê°ì§€! ìºì‹œë¥¼ í´ë¦¬ì–´í•©ë‹ˆë‹¤.")
            # ìºì‹œ í´ë¦¬ì–´
            from react_agent.cache_manager import get_cache_manager
            cache_manager = get_cache_manager()
            cache_manager.clear(prefix="rag")
            cache_manager.clear(prefix="llm")
            # ìˆ˜ì • ì‹œê°„ ì—…ë°ì´íŠ¸
            self._kb_last_modified = current_time

        return changed

    def _extract_section_title(self, content: str, chunk_start_idx: int) -> str:
        """ì²­í¬ê°€ ì†í•œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜)"""
        # ì²­í¬ ì‹œì‘ ìœ„ì¹˜ ì´ì „ì˜ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ í—¤ë” ì°¾ê¸°
        text_before = content[:chunk_start_idx]
        lines = text_before.split('\n')

        # ì—­ìˆœìœ¼ë¡œ ìˆœíšŒí•˜ë©° ë§ˆí¬ë‹¤ìš´ í—¤ë” ì°¾ê¸°
        for line in reversed(lines):
            line = line.strip()
            if line.startswith('#'):
                # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°í•˜ê³  ì œëª©ë§Œ ë°˜í™˜
                return line.lstrip('#').strip()

        return ""  # í—¤ë” ì—†ìŒ

    def _extract_keywords_from_text(self, text: str, max_keywords: int = 5) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜)"""
        # ë¶ˆìš©ì–´ ì œê±° ë° ëª…ì‚¬ ìœ„ì£¼ ì¶”ì¶œ
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
                     'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì•Šë‹¤', 'ê°™ë‹¤', 'ìœ„í•´', 'ëŒ€í•œ', 'í†µí•´', 'ë”°ë¼'}

        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ë¶ˆìš©ì–´ ì œê±°
        words = text.split()
        keywords = []

        for word in words:
            # ê¸¸ì´ 2 ì´ìƒ, ë¶ˆìš©ì–´ ì•„ë‹Œ ê²½ìš°
            if len(word) >= 2 and word not in stopwords:
                # íŠ¹ìˆ˜ë¬¸ì ì œê±°
                clean_word = ''.join(c for c in word if c.isalnum() or c in ['_', '-'])
                if clean_word and clean_word not in keywords:
                    keywords.append(clean_word)
                    if len(keywords) >= max_keywords:
                        break

        return keywords

    def _load_documents(self) -> List[Document]:
        """ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹ (ë©”íƒ€ë°ì´í„° ê°•í™”)"""
        documents = []

        if not self.knowledge_base_path.exists():
            logger.warning(f"ì§€ì‹ë² ì´ìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.knowledge_base_path}")
            return documents
        
        # ë¬¸ì„œ íŒŒì‹± í•¨ìˆ˜ë“¤
        def parse_text_file(file_path: Path) -> str:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            except Exception as e:
                logger.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path}): {e}")
                return ""
        
        def parse_pdf(file_path: Path) -> str:
            try:
                from pypdf import PdfReader
                reader = PdfReader(file_path)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
            except ImportError:
                logger.warning("pypdfê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                return ""
            except Exception as e:
                logger.error(f"PDF íŒŒì‹± ì‹¤íŒ¨ ({file_path}): {e}")
                return ""
        
        def parse_docx(file_path: Path) -> str:
            try:
                from docx import Document as DocxDocument
                doc = DocxDocument(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text
            except ImportError:
                logger.warning("python-docxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DOCX íŒŒì¼ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                return ""
            except Exception as e:
                logger.error(f"DOCX íŒŒì‹± ì‹¤íŒ¨ ({file_path}): {e}")
                return ""
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì ë° íŒŒì„œ ë§¤í•‘
        parsers = {
            '.txt': parse_text_file,
            '.md': parse_text_file,
            '.pdf': parse_pdf,
            '.docx': parse_docx,
        }
        
        # ëª¨ë“  ë¬¸ì„œ íŒŒì¼ ì°¾ê¸°
        for ext, parser_func in parsers.items():
            for file_path in self.knowledge_base_path.rglob(f"*{ext}"):
                try:
                    logger.info(f"ë¬¸ì„œ ë¡œë“œ ì¤‘: {file_path.name}")
                    
                    # ë¬¸ì„œ íŒŒì‹±
                    content = parser_func(file_path)
                    if not content.strip():
                        logger.warning(f"ë¹ˆ ë¬¸ì„œ: {file_path.name}")
                        continue
                    
                    # í…ìŠ¤íŠ¸ ë¶„í• 
                    chunks = self.text_splitter.split_text(content)

                    # Document ê°ì²´ ìƒì„± (ê°•í™”ëœ ë©”íƒ€ë°ì´í„°)
                    for i, chunk in enumerate(chunks):
                        # ì²­í¬ ìœ„ì¹˜ ì •ë³´
                        if len(chunks) == 1:
                            position = "full"
                        elif i == 0:
                            position = "beginning"
                        elif i == len(chunks) - 1:
                            position = "end"
                        else:
                            position = "middle"

                        # ì„¹ì…˜ ì œëª© ì¶”ì¶œ (ì²­í¬ ë‚´ ì²« í—¤ë” ë˜ëŠ” ì²« ì¤„)
                        section_title = ""
                        chunk_lines = chunk.split('\n')
                        for line in chunk_lines[:3]:  # ì²˜ìŒ 3ì¤„ í™•ì¸
                            line = line.strip()
                            if line.startswith('#'):
                                section_title = line.lstrip('#').strip()
                                break

                        # í‚¤ì›Œë“œ ì¶”ì¶œ (ì²­í¬ ë‚´ìš©ì—ì„œ)
                        keywords = self._extract_keywords_from_text(chunk, max_keywords=5)

                        doc = Document(
                            page_content=chunk,
                            metadata={
                                'source': str(file_path),
                                'filename': file_path.name,
                                'extension': ext,
                                'chunk_index': i,
                                'total_chunks': len(chunks),
                                'position': position,  # beginning/middle/end/full
                                'chunk_length': len(chunk),
                                'section_title': section_title,  # ì„¹ì…˜ ì œëª©
                                'keywords': ', '.join(keywords),  # í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                            }
                        )
                        documents.append(doc)
                    
                    logger.info(f"âœ“ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {file_path.name} ({len(chunks)}ê°œ ì²­í¬)")
                    
                except Exception as e:
                    logger.error(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
                    continue
        
        logger.info(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
        return documents
    
    def _build_vectorstore_if_needed(self) -> bool:
        """ë²¡í„° DBê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ êµ¬ì¶•"""
        # ì´ë¯¸ ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if self._vectorstore is not None:
            return True
        
        # ë²¡í„° DBê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¡œë“œë§Œ
        if self.chroma_db_path.exists() and any(self.chroma_db_path.iterdir()):
            return False
        
        # ì§€ì‹ë² ì´ìŠ¤ì— ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
        if not self.knowledge_base_path.exists():
            logger.warning(f"ì§€ì‹ë² ì´ìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.knowledge_base_path}")
            return False
        
        # ë¬¸ì„œ ì°¾ê¸°
        has_documents = False
        for ext in ['.txt', '.md', '.pdf', '.docx']:
            if any(self.knowledge_base_path.rglob(f"*{ext}")):
                has_documents = True
                break
        
        if not has_documents:
            logger.warning("ì§€ì‹ë² ì´ìŠ¤ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„° DBë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ë²¡í„° DB ìë™ êµ¬ì¶•
        logger.info("ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        try:
            documents = self._load_documents()
            
            if not documents:
                logger.warning("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # Chroma DB ìƒì„±
            logger.info(f"ë²¡í„° DB êµ¬ì¶• ì¤‘... ({len(documents)}ê°œ ë¬¸ì„œ ì²­í¬)")
            self._vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=str(self.chroma_db_path)
            )
            
            logger.info(f"âœ“ ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            logger.error(f"ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
    
    @property
    def vectorstore(self) -> Optional[Chroma]:
        """ë²¡í„° ìŠ¤í† ì–´ ì§€ì—° ë¡œë”© ë° ìë™ êµ¬ì¶•"""
        if self._vectorstore is None and self.available:
            try:
                # ë²¡í„° DBê°€ ì—†ìœ¼ë©´ ìë™ êµ¬ì¶• ì‹œë„
                self._build_vectorstore_if_needed()
                
                # ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ êµ¬ì¶•ëœ DB ì‚¬ìš©
                if self.chroma_db_path.exists() and any(self.chroma_db_path.iterdir()):
                    if self._vectorstore is None:
                        self._vectorstore = Chroma(
                            persist_directory=str(self.chroma_db_path),
                            embedding_function=self.embeddings
                        )
                    logger.info("ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")
                else:
                    logger.warning("ë²¡í„° DBê°€ ì•„ì§ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return self._vectorstore

    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í•  (í•œêµ­ì–´/ì˜ì–´ ì§€ì›)"""
        # ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ í† í¬ë‚˜ì´ì§•
        # ë” ì •êµí•œ í† í¬ë‚˜ì´ì§•ì´ í•„ìš”í•˜ë©´ konlpy ë“± ì‚¬ìš© ê°€ëŠ¥
        import re
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ)
        tokens = re.findall(r'[ê°€-í£a-z0-9]+', text)
        return tokens

    def _build_bm25_index(self) -> bool:
        """BM25 ì¸ë±ìŠ¤ ë¹Œë“œ"""
        if not self.available:
            return False

        try:
            # ë²¡í„° DBê°€ ìˆìœ¼ë©´ ê±°ê¸°ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            if self.vectorstore is not None:
                # Chroma DBì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                collection = self.vectorstore._collection
                results = collection.get(include=['documents', 'metadatas'])

                if not results or not results['documents']:
                    logger.warning("BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return False

                # Document ê°ì²´ë¡œ ë³€í™˜
                self._bm25_documents = []
                for i, (doc_text, metadata) in enumerate(zip(results['documents'], results['metadatas'])):
                    doc = Document(page_content=doc_text, metadata=metadata or {})
                    self._bm25_documents.append(doc)

            else:
                # ë²¡í„° DBê°€ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë“œ
                self._bm25_documents = self._load_documents()

            if not self._bm25_documents:
                logger.warning("BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            # ê° ë¬¸ì„œë¥¼ í† í°í™”
            tokenized_corpus = [
                self._tokenize(doc.page_content)
                for doc in self._bm25_documents
            ]

            # BM25 ì¸ë±ìŠ¤ ìƒì„±
            self._bm25_index = BM25Okapi(tokenized_corpus)
            logger.info(f"âœ“ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self._bm25_documents)}ê°œ ë¬¸ì„œ")
            return True

        except Exception as e:
            logger.error(f"BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False

    @property
    def bm25_index(self) -> Optional['BM25Okapi']:
        """BM25 ì¸ë±ìŠ¤ ì§€ì—° ë¡œë”©"""
        if self._bm25_index is None and self.available:
            self._build_bm25_index()
        return self._bm25_index

    def _extract_keywords_from_query(self, query: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ"""
        try:
            from langchain_anthropic import ChatAnthropic
            from langchain_core.messages import HumanMessage

            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œìš© Claude ëª¨ë¸
            llm = ChatAnthropic(
                model="claude-haiku-4-5",  # ìµœì‹  Haiku 4.5 ëª¨ë¸
                temperature=0
            )

            # í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (ë” ë§ì€ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ë„ë¡ ê°œì„ )
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ì¡°ì‚¬, ì˜ë¬¸ì‚¬, ìš”ì²­ì–´ëŠ” ì œê±°í•˜ê³  ëª…ì‚¬ ìœ„ì£¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
ì¤‘ìš”í•œ í‚¤ì›Œë“œëŠ” ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”. ìµœì†Œ 3-5ê°œ ì´ìƒì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

í•µì‹¬ í‚¤ì›Œë“œ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„, ìµœì†Œ 3ê°œ ì´ìƒ):"""

            response = llm.invoke([HumanMessage(content=prompt)])
            keywords = response.content.strip()

            # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì´ìƒí•˜ë©´ ì›ë³¸ ì‚¬ìš©
            if len(keywords) > len(query) or not keywords:
                logger.debug(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: '{query}'")
                return query

            logger.info(f"ì¿¼ë¦¬ ìš”ì•½: '{query}' -> '{keywords}'")
            return keywords

        except Exception as e:
            logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return query

    def search_documents(self, query: str, k: int = 3, similarity_threshold: float = 0.7) -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ (í‚¤ì›Œë“œ ì¶”ì¶œ + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            similarity_threshold: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.7, ì—„ê²©í•œ í•„í„°ë§)

        Returns:
            ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        """
        if not self.available or self.vectorstore is None:
            return []

        # ì§€ì‹ë² ì´ìŠ¤ ë³€ê²½ í™•ì¸ (ë³€ê²½ë˜ë©´ ìë™ìœ¼ë¡œ ìºì‹œ í´ë¦¬ì–´ë¨)
        self._check_kb_changed()

        # ìºì‹œ í™•ì¸ (ì¿¼ë¦¬ + íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ìºì‹±)
        cache_manager = get_cache_manager()
        cache_content = f"{query}|k={k}|threshold={similarity_threshold}"
        cached_result = cache_manager.get("rag", cache_content)
        if cached_result is not None:
            return cached_result

        try:
            # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword_query = self._extract_keywords_from_query(query)
            logger.info(f"[ê²€ìƒ‰] ì›ë³¸ ì¿¼ë¦¬: '{query}' -> í‚¤ì›Œë“œ: '{keyword_query}'")

            # ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ìˆ˜ í™•ì¸
            try:
                total_docs = self.vectorstore._collection.count()
                logger.info(f"ğŸ“š ì§€ì‹ë² ì´ìŠ¤: ì´ {total_docs}ê°œ ë¬¸ì„œ ì²­í¬")
            except:
                logger.warning("âš ï¸ ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ìˆ˜ í™•ì¸ ì‹¤íŒ¨")

            # í‚¤ì›Œë“œì™€ ì›ë³¸ ì¿¼ë¦¬ ëª¨ë‘ë¡œ ê²€ìƒ‰í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ í™•ë³´
            # í‚¤ì›Œë“œê°€ ì›ë³¸ê³¼ ë‹¤ë¥´ë©´ ë‘ ë²ˆ ê²€ìƒ‰, ê°™ìœ¼ë©´ í•œ ë²ˆë§Œ ê²€ìƒ‰
            all_docs_with_scores = []
            seen_doc_ids = set()
            original_docs_count = 0

            # 1. í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            keyword_docs = self.vectorstore.similarity_search_with_score(keyword_query, k=k * 3)
            logger.info(f"ğŸ” í‚¤ì›Œë“œ '{keyword_query}' ê²€ìƒ‰: {len(keyword_docs)}ê°œ ê²°ê³¼")

            for doc, score in keyword_docs:
                doc_id = (doc.metadata.get('source', ''), doc.metadata.get('chunk_index', 0))
                if doc_id not in seen_doc_ids:
                    all_docs_with_scores.append((doc, score))
                    seen_doc_ids.add(doc_id)

            # 2. ì›ë³¸ ì¿¼ë¦¬ë¡œë„ ê²€ìƒ‰ (í‚¤ì›Œë“œì™€ ë‹¤ë¥¼ ê²½ìš°)
            if keyword_query.lower() != query.lower():
                original_docs = self.vectorstore.similarity_search_with_score(query, k=k * 3)
                original_docs_count = len(original_docs)
                logger.info(f"ğŸ” ì›ë³¸ '{query}' ê²€ìƒ‰: {len(original_docs)}ê°œ ê²°ê³¼")
                for doc, score in original_docs:
                    doc_id = (doc.metadata.get('source', ''), doc.metadata.get('chunk_index', 0))
                    if doc_id not in seen_doc_ids:
                        all_docs_with_scores.append((doc, score))
                        seen_doc_ids.add(doc_id)

            # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬ (ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ)
            all_docs_with_scores.sort(key=lambda x: x[1])
            docs_with_scores = all_docs_with_scores[:k * 3]  # ìƒìœ„ k*3ê°œë§Œ ì‚¬ìš©

            # ìƒìœ„ 5ê°œ ê²°ê³¼ì˜ ì‹¤ì œ ì ìˆ˜ ì¶œë ¥ (ì„ê³„ê°’ í•„í„°ë§ ì „)
            logger.info(f"ğŸ“Š ìƒìœ„ {min(5, len(docs_with_scores))}ê°œ ë¬¸ì„œ (í•„í„°ë§ ì „):")
            for idx, (doc, distance) in enumerate(docs_with_scores[:5]):
                similarity = 1.0 - distance if distance <= 2.0 else max(0.0, 1.0 - (distance / 2.0))
                filename = doc.metadata.get('filename', 'unknown')
                preview = doc.page_content[:50].replace('\n', ' ')
                logger.info(f"  #{idx+1}: {filename} (ê±°ë¦¬: {distance:.4f}, ìœ ì‚¬ë„: {similarity:.4f}) - {preview}...")

            filtered_docs = []
            seen_keys = set()  # ì¤‘ë³µ ì œê±°ìš© (source + chunk_index ì¡°í•©)
            rejected_count = 0

            for idx, (doc, distance) in enumerate(docs_with_scores):
                # Chroma DBì˜ distance ì²˜ë¦¬
                # ChromaëŠ” L2 ê±°ë¦¬ ë˜ëŠ” ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
                # ì •ê·œí™”ëœ ë²¡í„°ì˜ ê²½ìš° ì½”ì‚¬ì¸ ê±°ë¦¬: 0 ~ 2 (ê°’ì´ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = 1 - ì½”ì‚¬ì¸ ê±°ë¦¬

                # distanceê°€ ìŒìˆ˜ì´ê±°ë‚˜ 2ë³´ë‹¤ í¬ë©´ ì´ìƒí•œ ê°’ì´ë¯€ë¡œ ì²˜ë¦¬
                if distance < 0:
                    similarity = 1.0  # ì™„ì „ ì¼ì¹˜ë¡œ ê°„ì£¼
                elif distance > 2.0:
                    similarity = max(0.0, 1.0 - (distance / 2.0))  # ì •ê·œí™”
                else:
                    similarity = 1.0 - distance

                # ìœ ì‚¬ë„ ì„ê³„ê°’ í™•ì¸
                if similarity < similarity_threshold:
                    rejected_count += 1
                    continue
                
                # ì¤‘ë³µ ì œê±°: ê°™ì€ sourceì™€ chunk_index ì¡°í•©ì€ ì œì™¸
                source = doc.metadata.get('source', 'unknown')
                chunk_index = doc.metadata.get('chunk_index', 0)
                doc_key = (source, chunk_index)
                
                if doc_key in seen_keys:
                    continue
                
                seen_keys.add(doc_key)
                
                filtered_docs.append({
                    'content': doc.page_content,
                    'source': source,
                    'filename': doc.metadata.get('filename', 'unknown'),
                    'chunk_index': chunk_index,
                    'similarity': similarity
                })

                # ìµœëŒ€ kê°œê¹Œì§€ë§Œ ìˆ˜ì§‘ (ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ)
                if len(filtered_docs) >= k:
                    break

            if not filtered_docs:
                logger.warning(
                    f"âŒ ì„ê³„ê°’ {similarity_threshold} ë¯¸ë§Œ: "
                    f"{len(docs_with_scores)}ê°œ ê²°ê³¼ ëª¨ë‘ ì œì™¸ë¨"
                )
                # ë¹ˆ ê²°ê³¼ë„ ìºì‹± (ë¶ˆí•„ìš”í•œ ì¬ê²€ìƒ‰ ë°©ì§€, TTLì€ ì§§ê²Œ)
                cache_manager.set("rag", cache_content, [], ttl=3600)  # 1ì‹œê°„
                return []

            logger.info(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_docs)}ê°œ ì„ íƒ, {rejected_count}ê°œ ì œì™¸")

            # ì„ê³„ê°’ì„ ë„˜ê¸´ ë¬¸ì„œë“¤ì˜ ìœ ì‚¬ë„ ë¡œê¹…
            logger.info(f"ğŸ“š ìµœì¢… ê²°ê³¼:")
            for idx, doc in enumerate(filtered_docs[:5]):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                logger.info(f"  #{idx+1}: {doc['filename']} (ìœ ì‚¬ë„: {doc['similarity']:.3f})")

            # ê²€ìƒ‰ ê²°ê³¼ ìºì‹± (24ì‹œê°„)
            cache_manager.set("rag", cache_content, filtered_docs)

            return filtered_docs
            
        except Exception as e:
            logger.error(f"[ê²€ìƒ‰] ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []

    def search_documents_hybrid(
        self,
        query: str,
        k: int = 3,
        alpha: float = 0.5,
        similarity_threshold: float = 0.6
    ) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ë²¡í„° ê²€ìƒ‰)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            alpha: ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ê°’ 0.5)
                   - 1.0: 100% ë²¡í„° ê²€ìƒ‰
                   - 0.0: 100% BM25 ê²€ìƒ‰
                   - 0.5: 50% ë²¡í„° + 50% BM25
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬)
        """
        if not self.available:
            return []

        # ì§€ì‹ë² ì´ìŠ¤ ë³€ê²½ í™•ì¸
        self._check_kb_changed()

        # ìºì‹œ í™•ì¸
        cache_manager = get_cache_manager()
        cache_content = f"hybrid|{query}|k={k}|alpha={alpha}|threshold={similarity_threshold}"
        cached_result = cache_manager.get("rag", cache_content)
        if cached_result is not None:
            return cached_result

        try:
            # ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ìˆ˜ í™•ì¸
            try:
                total_docs = self.vectorstore._collection.count()
                logger.info(f"ğŸ“š ì§€ì‹ë² ì´ìŠ¤: ì´ {total_docs}ê°œ ë¬¸ì„œ ì²­í¬")
            except:
                logger.warning("âš ï¸ ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ìˆ˜ í™•ì¸ ì‹¤íŒ¨")

            # 1. ë²¡í„° ê²€ìƒ‰
            vector_results = {}
            if self.vectorstore is not None:
                vector_docs = self.vectorstore.similarity_search_with_score(query, k=k * 3)
                logger.info(f"ğŸ” ë²¡í„° ê²€ìƒ‰: {len(vector_docs)}ê°œ ê²°ê³¼")
                for doc, distance in vector_docs:
                    doc_key = (doc.metadata.get('source', ''), doc.metadata.get('chunk_index', 0))
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (0~1 ë²”ìœ„)
                    similarity = max(0.0, 1.0 - min(distance, 2.0))
                    vector_results[doc_key] = {
                        'doc': doc,
                        'score': similarity
                    }

            # 2. BM25 ê²€ìƒ‰
            bm25_results = {}
            if self.bm25_index is not None and len(self._bm25_documents) > 0:
                # ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§•
                tokenized_query = self._tokenize(query)
                # BM25 ì ìˆ˜ ê³„ì‚°
                bm25_scores = self.bm25_index.get_scores(tokenized_query)

                # ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ)
                max_score = max(bm25_scores) if len(bm25_scores) > 0 and max(bm25_scores) > 0 else 1.0
                normalized_scores = [score / max_score for score in bm25_scores]

                # ìƒìœ„ k*3ê°œë§Œ ì„ íƒ
                top_indices = np.argsort(normalized_scores)[::-1][:k * 3]
                logger.info(f"ğŸ” BM25 ê²€ìƒ‰: {len(top_indices)}ê°œ ê²°ê³¼")

                for idx in top_indices:
                    doc = self._bm25_documents[idx]
                    doc_key = (doc.metadata.get('source', ''), doc.metadata.get('chunk_index', 0))
                    bm25_results[doc_key] = {
                        'doc': doc,
                        'score': normalized_scores[idx]
                    }

            # 3. ì ìˆ˜ ê²°í•© (alpha ê°€ì¤‘ì¹˜)
            combined_results = {}
            all_doc_keys = set(vector_results.keys()) | set(bm25_results.keys())

            for doc_key in all_doc_keys:
                vector_score = vector_results.get(doc_key, {}).get('score', 0.0)
                bm25_score = bm25_results.get(doc_key, {}).get('score', 0.0)

                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
                hybrid_score = alpha * vector_score + (1.0 - alpha) * bm25_score

                # ë¬¸ì„œ ê°ì²´ ê°€ì ¸ì˜¤ê¸° (ë²¡í„° ìš°ì„ , ì—†ìœ¼ë©´ BM25)
                doc = vector_results.get(doc_key, bm25_results.get(doc_key, {})).get('doc')

                if doc is not None:
                    combined_results[doc_key] = {
                        'doc': doc,
                        'hybrid_score': hybrid_score,
                        'vector_score': vector_score,
                        'bm25_score': bm25_score
                    }

            # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
            sorted_results = sorted(
                combined_results.items(),
                key=lambda x: x[1]['hybrid_score'],
                reverse=True
            )

            # ìƒìœ„ 5ê°œ ê²°ê³¼ì˜ ì‹¤ì œ ì ìˆ˜ ì¶œë ¥ (ì„ê³„ê°’ í•„í„°ë§ ì „)
            logger.info(f"ğŸ“Š ìƒìœ„ {min(5, len(sorted_results))}ê°œ ë¬¸ì„œ (í•„í„°ë§ ì „):")
            for idx, (doc_key, result) in enumerate(sorted_results[:5]):
                doc = result['doc']
                filename = doc.metadata.get('filename', 'unknown')
                preview = doc.page_content[:50].replace('\n', ' ')
                logger.info(
                    f"  #{idx+1}: {filename} "
                    f"(hybrid: {result['hybrid_score']:.3f} = "
                    f"vector: {result['vector_score']:.3f} + bm25: {result['bm25_score']:.3f}) "
                    f"- {preview}..."
                )

            # 5. í•„í„°ë§ ë° ê²°ê³¼ ìƒì„±
            filtered_docs = []
            rejected_count = 0
            for idx, (doc_key, result) in enumerate(sorted_results):
                hybrid_score = result['hybrid_score']

                # ì„ê³„ê°’ í™•ì¸
                if hybrid_score < similarity_threshold:
                    rejected_count += 1
                    continue

                doc = result['doc']
                filtered_docs.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'unknown'),
                    'filename': doc.metadata.get('filename', 'unknown'),
                    'chunk_index': doc.metadata.get('chunk_index', 0),
                    'similarity': hybrid_score,
                    'vector_score': result['vector_score'],
                    'bm25_score': result['bm25_score']
                })

                # ìµœëŒ€ kê°œê¹Œì§€
                if len(filtered_docs) >= k:
                    break

            if not filtered_docs:
                logger.warning(
                    f"âŒ ì„ê³„ê°’ {similarity_threshold} ë¯¸ë§Œ: "
                    f"{len(sorted_results)}ê°œ ê²°ê³¼ ëª¨ë‘ ì œì™¸ë¨"
                )
                cache_manager.set("rag", cache_content, [], ttl=3600)
                return []

            logger.info(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_docs)}ê°œ ì„ íƒ, {rejected_count}ê°œ ì œì™¸")

            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            logger.info(f"ğŸ“š ìµœì¢… ê²°ê³¼ (í•˜ì´ë¸Œë¦¬ë“œ, alpha={alpha}):")
            for idx, doc in enumerate(filtered_docs[:5]):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                logger.info(
                    f"  #{idx+1}: {doc['filename']} "
                    f"(hybrid: {doc['similarity']:.3f} = vector: {doc['vector_score']:.3f} + bm25: {doc['bm25_score']:.3f})"
                )

            # ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
            cache_manager.set("rag", cache_content, filtered_docs)
            return filtered_docs

        except Exception as e:
            logger.error(f"[í•˜ì´ë¸Œë¦¬ë“œ] ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []


# ì „ì—­ RAG ë„êµ¬ ì¸ìŠ¤í„´ìŠ¤
_rag_tool: Optional[RAGTool] = None


def get_rag_tool() -> RAGTool:
    """RAG ë„êµ¬ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_tool
    if _rag_tool is None:
        _rag_tool = RAGTool()
    return _rag_tool

