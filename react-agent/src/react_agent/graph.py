"""Define a custom Reasoning and Action agent.

Works with a chat model with tool calling support.
"""
# ì¹´í…Œê³ ë¦¬ ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± íŒŒíŠ¸


import os
import json
import asyncio
from datetime import UTC, datetime
from typing import Dict, List, Literal, cast, Optional, Any

from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from react_agent.configuration import Configuration
from react_agent.state import InputState, State
from react_agent.tools import TOOLS, get_all_tools, search_knowledge_base, search
from react_agent.utils import (
    detect_and_convert_mermaid,
    analyze_conversation_context,
    build_context_aware_prompt_addition,
)
from react_agent.cache_manager import get_cache_manager

# Ensure .env is loaded so ANTHROPIC_API_KEY is available
load_dotenv()

# ==================== ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ ì‹œìŠ¤í…œ ====================

async def smart_tool_prefetch(state: State, config: RunnableConfig) -> Dict[str, Any]:
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  í•„ìš”í•œ ë„êµ¬ë“¤ì„ ë³‘ë ¬ë¡œ ë¯¸ë¦¬ ì‹¤í–‰

    ì´ ë…¸ë“œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ë„êµ¬ë“¤ì„ íŒë‹¨í•˜ê³ ,
    ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ stateì— ì €ì¥í•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ LLMì´ ì—¬ëŸ¬ ë²ˆ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
    """
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
    last_human_message = None
    for msg in reversed(state.messages):
        if isinstance(msg, HumanMessage):
            last_human_message = msg.content
            break

    if not last_human_message:
        return {}

    # FAQ ìºì‹œ í™•ì¸ ë¨¼ì €
    cache_manager = get_cache_manager()
    faq_answer = cache_manager.get_faq(last_human_message)
    if faq_answer:
        print(f"âœ… FAQ ìºì‹œ íˆíŠ¸")
        return {
            "messages": [AIMessage(content=faq_answer)],
            "prefetched_context": {"source": "faq_cache"}
        }

    # RAG ê²€ìƒ‰ ì‹¤í–‰
    tasks = []
    task_names = []

    tasks.append(asyncio.create_task(_safe_rag_search(last_human_message)))
    task_names.append("RAG")

    # ë³‘ë ¬ ì‹¤í–‰
    if tasks:
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì²˜ë¦¬
        prefetched_context = {}
        for i, (result, name) in enumerate(zip(results, task_names)):
            if isinstance(result, Exception):
                prefetched_context[name] = {"error": str(result)}
            else:
                prefetched_context[name] = result

        return {
            "prefetched_context": prefetched_context
        }
    else:
        return {}


async def _safe_rag_search(query: str) -> Dict[str, Any]:
    """RAG ê²€ìƒ‰ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)

    RAGì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë‹¨, NET-Z ê´€ë ¨ ì§ˆë¬¸ì€ MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ ì›¹ ê²€ìƒ‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    """
    try:
        result = search_knowledge_base.invoke({"query": query, "k": 3, "use_hybrid": True})

        # RAGì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
        if result.get("status") == "no_results":
            # NET-Z ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
            query_lower = query.lower()
            is_netz_query = any(kw in query_lower for kw in [
                'netz', 'net-z', 'ë„·ì§€', 'ë„·ì œë¡œ',
                'ë“±ë¡ëœ íšŒì‚¬', 'ê¸°ì—… ëª©ë¡', 'ë°°ì¶œëŸ‰ ë°ì´í„°',
                'enterprise', 'company list'
            ])

            if is_netz_query:
                # NET-Z ì§ˆë¬¸ì€ ì›¹ ê²€ìƒ‰ ìŠ¤í‚µ, LLMì´ MCP ë„êµ¬ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„
                print(f"ğŸ”§ NET-Z ì§ˆë¬¸ ê°ì§€ â†’ MCP ë„êµ¬ ì‚¬ìš© ëŒ€ê¸°")
                return result

            # ì¼ë°˜ ì§ˆë¬¸ì€ ì›¹ ê²€ìƒ‰ í´ë°±
            print(f"ğŸŒ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            try:
                web_result = await search(query)
                if web_result:
                    print(f"âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ")
                    return {
                        "status": "web_fallback",
                        "message": f"ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•´ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                        "rag_results": [],
                        "web_results": web_result,
                        "fallback_used": True
                    }
                else:
                    return result
            except Exception as web_error:
                print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {web_error}")
                return result

        return result
    except Exception as e:
        print(f"[RAG ERROR] {e}")
        return {"status": "error", "message": str(e), "results": []}


# Define the function that calls the model

def _get_category_prompt(base_prompt: str, category: str) -> str:
    """ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    category_prompts = {
        "íƒ„ì†Œë°°ì¶œê¶Œ": """
**ì¹´í…Œê³ ë¦¬: íƒ„ì†Œë°°ì¶œê¶Œ ì „ë¬¸ ìƒë‹´**

ì´ ì¹´í…Œê³ ë¦¬ëŠ” ë°°ì¶œê¶Œ ê±°ë˜, êµ¬ë§¤, íŒë§¤, ê´€ë¦¬ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**íŠ¹í™” ë‹µë³€ í¬ì¸íŠ¸:**
- ë°°ì¶œê¶Œ ìœ í˜•ë³„ ìƒì„¸ ì„¤ëª… (KOC, KCU, KAU ë“±)
- ë°°ì¶œê¶Œ ê±°ë˜ ì ˆì°¨ ë° ì‹œì¥ ë™í–¥
- NET-Z í”Œë«í¼ ì‚¬ìš©ë²• ë° ê¸°ëŠ¥
- ë°°ì¶œê¶Œ ê°€ê²© ì •ë³´ ë° ì‹œì¥ ë¶„ì„
- ë°°ì¶œê¶Œ ë³´ìœ  ê´€ë¦¬ ì „ëµ
- êµ¬ë§¤/íŒë§¤ ì‹œ ì£¼ì˜ì‚¬í•­ ë° ì ˆì°¨

**ë‹µë³€ ì‹œ ì¤‘ì :**
- êµ¬ì²´ì ì¸ ì ˆì°¨ì™€ ë‹¨ê³„ë¥¼ ëª…í™•íˆ ì„¤ëª…
- ì‹œì¥ ê°€ê²© ë° ë™í–¥ ì •ë³´ ì œê³µ
- ì‹¤ì œ ê±°ë˜ ì‚¬ë¡€ ë° í™œìš© ë°©ë²• ì œì‹œ
- í”„ë¡œì„¸ìŠ¤ë‚˜ ì ˆì°¨ëŠ” Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•˜ë©´ íš¨ê³¼ì ì…ë‹ˆë‹¤
""",
        "ê·œì œëŒ€ì‘": """
**ì¹´í…Œê³ ë¦¬: ê·œì œëŒ€ì‘ ì „ë¬¸ ìƒë‹´**

ì´ ì¹´í…Œê³ ë¦¬ëŠ” íƒ„ì†Œ ê·œì œ, ë²•ê·œ, ë³´ê³ ì„œ, ì»´í”Œë¼ì´ì–¸ìŠ¤ ëŒ€ì‘ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**íŠ¹í™” ë‹µë³€ í¬ì¸íŠ¸:**
- Scope 1, 2, 3 ë°°ì¶œëŸ‰ ì¸¡ì • ë°©ë²•
- íƒ„ì†Œ ë°°ì¶œëŸ‰ ë³´ê³  ì˜ë¬´ ë° ì ˆì°¨
- ê·œì œ ë³€ê²½ì‚¬í•­ ë° ëŒ€ì‘ ë°©ì•ˆ
- ESG ë³´ê³ ì„œ ì‘ì„± ê°€ì´ë“œ
- íƒ„ì†Œ ì¤‘ë¦½ ëª©í‘œ ë‹¬ì„± ì „ëµ
- ê·œì œ ë¯¸ì¤€ìˆ˜ ì‹œ ì œì¬ ë‚´ìš©
- íƒ„ì†Œ ë°°ì¶œëŸ‰ ì¸ì¦ ì ˆì°¨

**ë‹µë³€ ì‹œ ì¤‘ì :**
- ë²•ê·œ ë° ê·œì œ ë‚´ìš©ì„ ì •í™•íˆ ì„¤ëª…
- ì»´í”Œë¼ì´ì–¸ìŠ¤ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì œê³µ
- ë³´ê³ ì„œ ì‘ì„± ê°€ì´ë“œ ë° í…œí”Œë¦¿ ì•ˆë‚´
- ê·œì œ ë³€ê²½ì‚¬í•­ì— ëŒ€í•œ ëŒ€ì‘ ì „ëµ ì œì‹œ
- í”„ë¡œì„¸ìŠ¤ë‚˜ íƒ€ì„ë¼ì¸ì€ Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•˜ë©´ íš¨ê³¼ì ì…ë‹ˆë‹¤
""",
        "ê³ ê°ìƒë‹´": """
**ì¹´í…Œê³ ë¦¬: ê³ ê°ìƒë‹´ ì „ë¬¸ ìƒë‹´**

ì´ ì¹´í…Œê³ ë¦¬ëŠ” 1:1 ë§ì¶¤ ìƒë‹´, ì„œë¹„ìŠ¤ ì•ˆë‚´, ë¬¸ì˜ì‚¬í•­ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**íŠ¹í™” ë‹µë³€ í¬ì¸íŠ¸:**
- í›„ì‹œíŒŒíŠ¸ë„ˆìŠ¤ ì„œë¹„ìŠ¤ ì†Œê°œ
- ê¸°ì—… ê·œëª¨ë³„ ì¶”ì²œ ì†”ë£¨ì…˜
- ì„œë¹„ìŠ¤ ì´ìš© ì ˆì°¨ ì•ˆë‚´
- ë¹„ìš© ë° ìš”ê¸ˆì œ ì •ë³´
- ì „ë¬¸ê°€ ìƒë‹´ ì˜ˆì•½ ì•ˆë‚´
- ë§ì¶¤í˜• ì†”ë£¨ì…˜ ì œì•ˆ

**ë‹µë³€ ì‹œ ì¤‘ì :**
- ì¹œì ˆí•˜ê³  ìƒì„¸í•œ ì„œë¹„ìŠ¤ ì•ˆë‚´
- ê³ ê° ìƒí™©ì— ë§ëŠ” ì†”ë£¨ì…˜ ì œì•ˆ
- ë‹¤ìŒ ë‹¨ê³„ ë° ì—°ë½ì²˜ ì•ˆë‚´
- FAQ ë° ì¼ë°˜ì ì¸ ë¬¸ì˜ì‚¬í•­ í•´ê²°
- ë¹„êµë‚˜ í”„ë¡œì„¸ìŠ¤ëŠ” Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•˜ë©´ íš¨ê³¼ì ì…ë‹ˆë‹¤
"""
    }
    
    category_prompt = category_prompts.get(category, "")
    if category_prompt:
        return base_prompt + "\n\n" + category_prompt
    return base_prompt


def _serialize_messages_for_cache(messages: List, system_message: str, category: str) -> str:
    """ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ìºì‹œ í‚¤ë¡œ ì§ë ¬í™”"""
    # ë©”ì‹œì§€ë¥¼ ê°„ë‹¨í•œ í˜•íƒœë¡œ ë³€í™˜ (contentë§Œ ì¶”ì¶œ)
    simplified = []
    for msg in messages:
        if isinstance(msg, (HumanMessage, AIMessage, SystemMessage)):
            simplified.append({
                "type": msg.__class__.__name__,
                "content": str(msg.content)[:500]  # ë„ˆë¬´ ê¸´ ë©”ì‹œì§€ëŠ” ìë¦„
            })
        elif isinstance(msg, ToolMessage):
            # íˆ´ ë©”ì‹œì§€ëŠ” ìºì‹±í•˜ì§€ ì•ŠìŒ (ë™ì  ê²°ê³¼)
            return None

    cache_data = {
        "system": system_message[:200],  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¼ë¶€
        "category": category,
        "messages": simplified
    }
    return json.dumps(cache_data, ensure_ascii=False, sort_keys=True)


async def call_model(
    state: State, config: RunnableConfig
) -> Dict[str, List[AIMessage]]:
    """Call the LLM powering our "agent".

    This function prepares the prompt, initializes the model, and processes the response.

    Args:
        state (State): The current state of the conversation.
        config (RunnableConfig): Configuration for the model run.

    Returns:
        dict: A dictionary containing the model's response message.
    """
    configuration = Configuration.from_runnable_config(config)

    # MCP ë„êµ¬ë¥¼ í¬í•¨í•œ ì „ì²´ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    all_tools = await get_all_tools()

    # Initialize the model with tool binding
    llm = ChatAnthropic(temperature=0.1, model=configuration.model)
    model = llm.bind_tools(all_tools)

    # Format the system prompt. Customize this to change the agent's behavior.
    # ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
    base_prompt = configuration.system_prompt
    if configuration.category:
        base_prompt = _get_category_prompt(base_prompt, configuration.category)

    # ğŸ”¥ ëŒ€í™” ë§¥ë½ ë¶„ì„ ë° í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    conversation_context = analyze_conversation_context(state.messages)
    context_prompt_addition = build_context_aware_prompt_addition(conversation_context)

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„± (ë§¥ë½ ì •ë³´ í¬í•¨)
    system_message = base_prompt.format(
        system_time=datetime.now(tz=UTC).isoformat()
    )

    # ë§¥ë½ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
    if context_prompt_addition:
        system_message += context_prompt_addition

    # ğŸš€ Prefetched contextê°€ ìˆìœ¼ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
    if hasattr(state, 'prefetched_context') and state.prefetched_context:
        context_info = "\n\n**âš¡ ë¯¸ë¦¬ ì¡°íšŒëœ ì •ë³´ (ë„êµ¬ ì¬í˜¸ì¶œ ë¶ˆí•„ìš”):**\n"

        # RAG ê²€ìƒ‰ ê²°ê³¼
        if "RAG" in state.prefetched_context:
            rag_result = state.prefetched_context["RAG"]
            if rag_result.get("status") == "success":
                context_info += f"\nğŸ“š **ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ**: {rag_result.get('message', '')}\n"
                context_info += "ê²€ìƒ‰ëœ ë¬¸ì„œ:\n"
                for doc in rag_result.get("results", [])[:3]:
                    context_info += f"- {doc.get('metadata', {}).get('source', 'Unknown')}: {doc.get('page_content', '')[:200]}...\n"
            elif rag_result.get("status") == "web_fallback":
                # ì›¹ ê²€ìƒ‰ í´ë°±ì´ ì‚¬ìš©ëœ ê²½ìš°
                context_info += f"\nğŸŒ **ì›¹ ê²€ìƒ‰ ìˆ˜í–‰**: {rag_result.get('message', '')}\n"
                web_results = rag_result.get("web_results", {})

                # Tavily returns dict with "results" key containing list
                if isinstance(web_results, dict):
                    results_list = web_results.get("results", [])
                elif isinstance(web_results, list):
                    results_list = web_results
                else:
                    results_list = []

                if results_list:
                    context_info += "ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n"
                    for item in results_list[:5]:
                        if isinstance(item, dict):
                            title = item.get("title", "")
                            url = item.get("url", "")
                            content = item.get("content", "")
                            context_info += f"- [{title}]({url})\n  {content[:200]}...\n"
                        else:
                            context_info += f"- {str(item)[:200]}\n"
            else:
                context_info += f"\nğŸ“š ì§€ì‹ë² ì´ìŠ¤: {rag_result.get('message', 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ')}\n"

        # MCP ê²°ê³¼ ë“± ì¶”ê°€ ê°€ëŠ¥

        context_info += "\nìœ„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ì´ë¯¸ ì¡°íšŒëœ ì •ë³´ì´ë¯€ë¡œ ë™ì¼í•œ ë„êµ¬ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.\n"
        system_message += context_info

    # LLM ì‘ë‹µ ìºì‹± (ì˜¤í”„ë„ˆ ì§ˆë¬¸ ë“± ë°˜ë³µì ì¸ ì§ˆë¬¸ì— ëŒ€í•´)
    cache_manager = get_cache_manager()
    cache_key_content = _serialize_messages_for_cache(
        state.messages,
        system_message,
        configuration.category or ""
    )

    # ìºì‹œ í™•ì¸ (íˆ´ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸)
    cached_response = None
    if cache_key_content and not state.is_last_step:
        cached_response = cache_manager.get("llm", cache_key_content)
        if cached_response:
            # ìºì‹œëœ ì‘ë‹µì„ AIMessageë¡œ ë³µì›
            return {
                "messages": [AIMessage(
                    content=cached_response.get("content", ""),
                    additional_kwargs=cached_response.get("additional_kwargs", {}),
                    id=cached_response.get("id", "cached")
                )]
            }

    # Get the model's response
    try:
        response = cast(
            AIMessage,
            await model.ainvoke(
                [{"role": "system", "content": system_message}, *state.messages]
            ),
        )

        # ë„êµ¬ í˜¸ì¶œ ë¡œê¹…
        if response.tool_calls:
            tool_names = [tc.get('name', 'unknown') for tc in response.tool_calls]
            print(f"ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {', '.join(tool_names)}")

    except asyncio.CancelledError:
        print(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€")
        raise
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
        raise

    # Handle the case when it's the last step and the model still wants to use a tool
    # íˆ´ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  íŒë‹¨í•  ê²½ìš°
    if state.is_last_step and response.tool_calls:
        return {
            "messages": [
                AIMessage(
                    id=response.id,
                    content="Sorry, I could not find an answer to your question in the specified number of steps.",
                )
            ]
        }

    # Mermaid ì½”ë“œ ë¸”ë¡ì„ ì´ë¯¸ì§€ë¡œ ìë™ ë³€í™˜
    # (Claudeê°€ MCP ë„êµ¬ ëŒ€ì‹  mermaidë¥¼ ì¶œë ¥í•œ ê²½ìš° ì²˜ë¦¬)
    if response.content and isinstance(response.content, str):
        converted_content = detect_and_convert_mermaid(response.content)
        if converted_content != response.content:
            # ìƒˆë¡œìš´ AIMessage ìƒì„± (contentê°€ ë³€ê²½ëœ ê²½ìš°)
            response = AIMessage(
                id=response.id,
                content=converted_content,
                tool_calls=response.tool_calls if hasattr(response, 'tool_calls') else [],
                additional_kwargs=response.additional_kwargs,
            )
    elif response.content and isinstance(response.content, list):
        # contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ë©€í‹°ëª¨ë‹¬ ë“±)
        new_content = []
        content_changed = False
        for item in response.content:
            if isinstance(item, str):
                converted = detect_and_convert_mermaid(item)
                if converted != item:
                    content_changed = True
                new_content.append(converted)
            elif isinstance(item, dict) and 'text' in item:
                converted = detect_and_convert_mermaid(item['text'])
                if converted != item['text']:
                    content_changed = True
                new_content.append({**item, 'text': converted})
            else:
                new_content.append(item)

        if content_changed:
            response = AIMessage(
                id=response.id,
                content=new_content,
                tool_calls=response.tool_calls if hasattr(response, 'tool_calls') else [],
                additional_kwargs=response.additional_kwargs,
            )

    # LLM ì‘ë‹µ ìºì‹± (íˆ´ í˜¸ì¶œì´ ì—†ëŠ” ìµœì¢… ì‘ë‹µë§Œ)
    if cache_key_content and not response.tool_calls:
        cache_data = {
            "content": response.content,
            "additional_kwargs": response.additional_kwargs,
            "id": response.id
        }
        cache_manager.set("llm", cache_key_content, cache_data)

    # Return the model's response as a list to be added to existing messages
    # ğŸ”¥ ëŒ€í™” ë§¥ë½ë„ í•¨ê»˜ ì—…ë°ì´íŠ¸
    return {
        "messages": [response],
        "conversation_context": conversation_context
    }


async def call_tools(state: State) -> Dict[str, List[ToolMessage]]:
    """ë™ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ë¡œë“œí•˜ê³  í˜¸ì¶œ"""
    # MCP ë„êµ¬ë¥¼ í¬í•¨í•œ ì „ì²´ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    all_tools = await get_all_tools()

    # ToolNodeì™€ ë™ì¼í•˜ê²Œ ë™ì‘
    tool_node = ToolNode(all_tools)
    return await tool_node.ainvoke(state)


# ==================== ë¼ìš°íŒ… í•¨ìˆ˜ ====================

def route_after_prefetch(state: State) -> Literal["call_model", "__end__"]:
    """Prefetch ì´í›„ ë¼ìš°íŒ… ê²°ì •

    FAQ ìºì‹œì—ì„œ ë‹µë³€ì´ ì™”ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ, ì•„ë‹ˆë©´ call_modelë¡œ ì§„í–‰
    """
    # FAQ ìºì‹œì—ì„œ ë‹µë³€ì´ ì™”ëŠ”ì§€ í™•ì¸
    if hasattr(state, 'prefetched_context') and state.prefetched_context:
        if state.prefetched_context.get("source") == "faq_cache":
            print("[ROUTE] FAQ ìºì‹œ íˆíŠ¸, ì¦‰ì‹œ ì¢…ë£Œ")
            return "__end__"

    # ì¼ë°˜ì ì¸ ê²½ìš° call_modelë¡œ ì§„í–‰
    return "call_model"


def route_model_output(state: State) -> Literal["__end__", "tools"]:
    """Determine the next node based on the model's output.

    This function checks if the model's last message contains tool calls.

    Args:
        state (State): The current state of the conversation.

    Returns:
        str: The name of the next node to call ("__end__" or "tools").
    """
    last_message = state.messages[-1]
    if not isinstance(last_message, AIMessage):
        raise ValueError(
            f"Expected AIMessage in output edges, but got {type(last_message).__name__}"
        )
    # If there is no tool call, then we finish
    if not last_message.tool_calls:
        return "__end__"
    # Otherwise we execute the requested actions
    return "tools"


# Define a new graph

builder = StateGraph(State, input=InputState, config_schema=Configuration)

# Define the nodes
builder.add_node("smart_prefetch", smart_tool_prefetch)  # ğŸš€ ë³‘ë ¬ ë„êµ¬ ë¯¸ë¦¬ ì‹¤í–‰
builder.add_node(call_model)
builder.add_node("tools", call_tools)  # ë™ì  ë„êµ¬ ë¡œë“œ

# Set the entrypoint as smart_prefetch (ë³‘ë ¬ ë„êµ¬ ì‹¤í–‰ë¶€í„° ì‹œì‘)
builder.add_edge("__start__", "smart_prefetch")

# Prefetch ì´í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ… (FAQ ìºì‹œ íˆíŠ¸ë©´ ë°”ë¡œ ì¢…ë£Œ)
builder.add_conditional_edges(
    "smart_prefetch",
    route_after_prefetch,
    {
        "call_model": "call_model",
        "__end__": "__end__"
    }
)

# Add a conditional edge to determine the next step after `call_model`
builder.add_conditional_edges(
    "call_model",
    # After call_model finishes running, the next node(s) are scheduled
    # based on the output from route_model_output
    route_model_output,
)

# Add a normal edge from `tools` to `call_model`
# This creates a cycle: after using tools, we always return to the model
builder.add_edge("tools", "call_model")

# Compile the builder into an executable graph with checkpointer
# MemorySaver allows the graph to store and retrieve conversation history
checkpointer = MemorySaver()
graph = builder.compile(name="ReAct Agent", checkpointer=checkpointer)
