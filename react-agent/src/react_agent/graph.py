"""Define a custom Reasoning and Action agent.

Works with a chat model with tool calling support.
"""
# ì¹´í…Œê³ ë¦¬ ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± íŒŒíŠ¸


import os
import json
from datetime import UTC, datetime
from typing import Dict, List, Literal, cast

from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from react_agent.configuration import Configuration
from react_agent.state import InputState, State
from react_agent.tools import TOOLS, get_all_tools
from react_agent.utils import (
    detect_and_convert_mermaid,
    analyze_conversation_context,
    build_context_aware_prompt_addition,
)
from react_agent.cache_manager import get_cache_manager

# Ensure .env is loaded so ANTHROPIC_API_KEY is available
load_dotenv()

# Define the function that calls the model

def _get_category_prompt(base_prompt: str, category: str) -> str:
    """ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    category_prompts = {
        "íƒ„ì†Œë°°ì¶œê¶Œ": """
**ì¹´í…Œê³ ë¦¬: íƒ„ì†Œë°°ì¶œê¶Œ ì „ë¬¸ ìƒë‹´**

ì´ ì¹´í…Œê³ ë¦¬ëŠ” ë°°ì¶œê¶Œ ê±°ëž˜, êµ¬ë§¤, íŒë§¤, ê´€ë¦¬ì— íŠ¹í™”ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

**íŠ¹í™” ë‹µë³€ í¬ì¸íŠ¸:**
- ë°°ì¶œê¶Œ ìœ í˜•ë³„ ìƒì„¸ ì„¤ëª… (KOC, KCU, KAU ë“±)
- ë°°ì¶œê¶Œ ê±°ëž˜ ì ˆì°¨ ë° ì‹œìž¥ ë™í–¥
- NET-Z í”Œëž«í¼ ì‚¬ìš©ë²• ë° ê¸°ëŠ¥
- ë°°ì¶œê¶Œ ê°€ê²© ì •ë³´ ë° ì‹œìž¥ ë¶„ì„
- ë°°ì¶œê¶Œ ë³´ìœ  ê´€ë¦¬ ì „ëžµ
- êµ¬ë§¤/íŒë§¤ ì‹œ ì£¼ì˜ì‚¬í•­ ë° ì ˆì°¨

**ë‹µë³€ ì‹œ ì¤‘ì :**
- êµ¬ì²´ì ì¸ ì ˆì°¨ì™€ ë‹¨ê³„ë¥¼ ëª…í™•ížˆ ì„¤ëª…
- ì‹œìž¥ ê°€ê²© ë° ë™í–¥ ì •ë³´ ì œê³µ
- ì‹¤ì œ ê±°ëž˜ ì‚¬ë¡€ ë° í™œìš© ë°©ë²• ì œì‹œ
- í”„ë¡œì„¸ìŠ¤ë‚˜ ì ˆì°¨ëŠ” Mermaid ë‹¤ì´ì–´ê·¸ëž¨ìœ¼ë¡œ ì‹œê°í™”í•˜ë©´ íš¨ê³¼ì ìž…ë‹ˆë‹¤
""",
        "ê·œì œëŒ€ì‘": """
**ì¹´í…Œê³ ë¦¬: ê·œì œëŒ€ì‘ ì „ë¬¸ ìƒë‹´**

ì´ ì¹´í…Œê³ ë¦¬ëŠ” íƒ„ì†Œ ê·œì œ, ë²•ê·œ, ë³´ê³ ì„œ, ì»´í”Œë¼ì´ì–¸ìŠ¤ ëŒ€ì‘ì— íŠ¹í™”ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

**íŠ¹í™” ë‹µë³€ í¬ì¸íŠ¸:**
- Scope 1, 2, 3 ë°°ì¶œëŸ‰ ì¸¡ì • ë°©ë²•
- íƒ„ì†Œ ë°°ì¶œëŸ‰ ë³´ê³  ì˜ë¬´ ë° ì ˆì°¨
- ê·œì œ ë³€ê²½ì‚¬í•­ ë° ëŒ€ì‘ ë°©ì•ˆ
- ESG ë³´ê³ ì„œ ìž‘ì„± ê°€ì´ë“œ
- íƒ„ì†Œ ì¤‘ë¦½ ëª©í‘œ ë‹¬ì„± ì „ëžµ
- ê·œì œ ë¯¸ì¤€ìˆ˜ ì‹œ ì œìž¬ ë‚´ìš©
- íƒ„ì†Œ ë°°ì¶œëŸ‰ ì¸ì¦ ì ˆì°¨

**ë‹µë³€ ì‹œ ì¤‘ì :**
- ë²•ê·œ ë° ê·œì œ ë‚´ìš©ì„ ì •í™•ížˆ ì„¤ëª…
- ì»´í”Œë¼ì´ì–¸ìŠ¤ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì œê³µ
- ë³´ê³ ì„œ ìž‘ì„± ê°€ì´ë“œ ë° í…œí”Œë¦¿ ì•ˆë‚´
- ê·œì œ ë³€ê²½ì‚¬í•­ì— ëŒ€í•œ ëŒ€ì‘ ì „ëžµ ì œì‹œ
- í”„ë¡œì„¸ìŠ¤ë‚˜ íƒ€ìž„ë¼ì¸ì€ Mermaid ë‹¤ì´ì–´ê·¸ëž¨ìœ¼ë¡œ ì‹œê°í™”í•˜ë©´ íš¨ê³¼ì ìž…ë‹ˆë‹¤
""",
        "ê³ ê°ìƒë‹´": """
**ì¹´í…Œê³ ë¦¬: ê³ ê°ìƒë‹´ ì „ë¬¸ ìƒë‹´**

ì´ ì¹´í…Œê³ ë¦¬ëŠ” 1:1 ë§žì¶¤ ìƒë‹´, ì„œë¹„ìŠ¤ ì•ˆë‚´, ë¬¸ì˜ì‚¬í•­ì— íŠ¹í™”ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

**íŠ¹í™” ë‹µë³€ í¬ì¸íŠ¸:**
- í›„ì‹œíŒŒíŠ¸ë„ˆìŠ¤ ì„œë¹„ìŠ¤ ì†Œê°œ
- ê¸°ì—… ê·œëª¨ë³„ ì¶”ì²œ ì†”ë£¨ì…˜
- ì„œë¹„ìŠ¤ ì´ìš© ì ˆì°¨ ì•ˆë‚´
- ë¹„ìš© ë° ìš”ê¸ˆì œ ì •ë³´
- ì „ë¬¸ê°€ ìƒë‹´ ì˜ˆì•½ ì•ˆë‚´
- ë§žì¶¤í˜• ì†”ë£¨ì…˜ ì œì•ˆ

**ë‹µë³€ ì‹œ ì¤‘ì :**
- ì¹œì ˆí•˜ê³  ìƒì„¸í•œ ì„œë¹„ìŠ¤ ì•ˆë‚´
- ê³ ê° ìƒí™©ì— ë§žëŠ” ì†”ë£¨ì…˜ ì œì•ˆ
- ë‹¤ìŒ ë‹¨ê³„ ë° ì—°ë½ì²˜ ì•ˆë‚´
- FAQ ë° ì¼ë°˜ì ì¸ ë¬¸ì˜ì‚¬í•­ í•´ê²°
- ë¹„êµë‚˜ í”„ë¡œì„¸ìŠ¤ëŠ” Mermaid ë‹¤ì´ì–´ê·¸ëž¨ìœ¼ë¡œ ì‹œê°í™”í•˜ë©´ íš¨ê³¼ì ìž…ë‹ˆë‹¤
"""
    }
    
    category_prompt = category_prompts.get(category, "")
    if category_prompt:
        return base_prompt + "\n\n" + category_prompt
    return base_prompt


def _serialize_messages_for_cache(messages: List, system_message: str, category: str) -> str:
    """ë©”ì‹œì§€ ížˆìŠ¤í† ë¦¬ë¥¼ ìºì‹œ í‚¤ë¡œ ì§ë ¬í™”"""
    # ë©”ì‹œì§€ë¥¼ ê°„ë‹¨í•œ í˜•íƒœë¡œ ë³€í™˜ (contentë§Œ ì¶”ì¶œ)
    simplified = []
    for msg in messages:
        if isinstance(msg, (HumanMessage, AIMessage, SystemMessage)):
            simplified.append({
                "type": msg.__class__.__name__,
                "content": str(msg.content)[:500]  # ë„ˆë¬´ ê¸´ ë©”ì‹œì§€ëŠ” ìžë¦„
            })
        elif isinstance(msg, ToolMessage):
            # íˆ´ ë©”ì‹œì§€ëŠ” ìºì‹±í•˜ì§€ ì•ŠìŒ (ë™ì  ê²°ê³¼)
            return None

    cache_data = {
        "system": system_message[:200],  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¼ë¶€
        "category": category,
        "messages": simplified
    }
    return json.dumps(cache_data, ensure_ascii=False, sort_keys=True)


async def call_model(
    state: State, config: RunnableConfig
) -> Dict[str, List[AIMessage]]:
    """Call the LLM powering our "agent".

    This function prepares the prompt, initializes the model, and processes the response.

    Args:
        state (State): The current state of the conversation.
        config (RunnableConfig): Configuration for the model run.

    Returns:
        dict: A dictionary containing the model's response message.
    """
    configuration = Configuration.from_runnable_config(config)

    # MCP ë„êµ¬ë¥¼ í¬í•¨í•œ ì „ì²´ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    all_tools = await get_all_tools()
    # Safely get tool names (handle both Tool objects and functions)
    tool_names = []
    for tool in all_tools:
        if hasattr(tool, 'name'):
            tool_names.append(tool.name)
        elif hasattr(tool, '__name__'):
            tool_names.append(tool.__name__)
        else:
            tool_names.append(str(type(tool).__name__))
    print(f"[CALL_MODEL] Loaded {len(all_tools)} tools: {tool_names}")

    # Initialize the model with tool binding. Change the model or add more tools here.
    # ChatAnthropic ê°ì²´ ìƒì„±
    # Enable streaming so LangGraph can emit token chunks during astream
    llm = ChatAnthropic(
        temperature=0.1,
        model=configuration.model,
        streaming=True
    )
    model = llm.bind_tools(all_tools)
    print(f"[CALL_MODEL] Model initialized with tools bound")

    # Format the system prompt. Customize this to change the agent's behavior.
    # ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
    base_prompt = configuration.system_prompt
    if configuration.category:
        base_prompt = _get_category_prompt(base_prompt, configuration.category)

    # ðŸ”¥ ëŒ€í™” ë§¥ë½ ë¶„ì„ ë° í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    conversation_context = analyze_conversation_context(state.messages)
    context_prompt_addition = build_context_aware_prompt_addition(conversation_context)

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„± (ë§¥ë½ ì •ë³´ í¬í•¨)
    system_message = base_prompt.format(
        system_time=datetime.now(tz=UTC).isoformat()
    )

    # ë§¥ë½ ì •ë³´ê°€ ìžˆìœ¼ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
    if context_prompt_addition:
        system_message += context_prompt_addition

    # LLM ì‘ë‹µ ìºì‹± (ì˜¤í”„ë„ˆ ì§ˆë¬¸ ë“± ë°˜ë³µì ì¸ ì§ˆë¬¸ì— ëŒ€í•´)
    cache_manager = get_cache_manager()
    cache_key_content = _serialize_messages_for_cache(
        state.messages,
        system_message,
        configuration.category or ""
    )

    # ìºì‹œ í™•ì¸ (íˆ´ í˜¸ì¶œì´ ìžˆëŠ” ê²½ìš°ëŠ” ì œì™¸)
    cached_response = None
    if cache_key_content and not state.is_last_step:
        cached_response = cache_manager.get("llm", cache_key_content)
        if cached_response:
            # ìºì‹œëœ ì‘ë‹µì„ AIMessageë¡œ ë³µì›
            return {
                "messages": [AIMessage(
                    content=cached_response.get("content", ""),
                    additional_kwargs=cached_response.get("additional_kwargs", {}),
                    id=cached_response.get("id", "cached")
                )]
            }

    # Get the model's response
    import asyncio
    try:
        response = cast(  # ì „ì²´ ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ íŽ¼ì³ì„œ aiì—ê²Œ ì „ë‹¬
            AIMessage,
            await model.ainvoke(
                [{"role": "system", "content": system_message}, *state.messages]
            ),
        ) # ainvokeëŠ” ëª¨ë¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜ë°›ëŠ” í•¨ìˆ˜
    except asyncio.CancelledError:
        print(f"[CALL_MODEL] Client disconnected during model invocation - propagating cancellation")
        # Re-raise to properly cleanup the entire chain
        # Don't return a message as the client has already disconnected
        raise
    except Exception as e:
        print(f"[CALL_MODEL ERROR] {type(e).__name__}: {e}")
        raise

    # Handle the case when it's the last step and the model still wants to use a tool
    # íˆ´ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  íŒë‹¨í•  ê²½ìš°
    if state.is_last_step and response.tool_calls:
        return {
            "messages": [
                AIMessage(
                    id=response.id,
                    content="Sorry, I could not find an answer to your question in the specified number of steps.",
                )
            ]
        }

    # Mermaid ì½”ë“œ ë¸”ë¡ì„ ì´ë¯¸ì§€ë¡œ ìžë™ ë³€í™˜
    # (Claudeê°€ MCP ë„êµ¬ ëŒ€ì‹  mermaidë¥¼ ì¶œë ¥í•œ ê²½ìš° ì²˜ë¦¬)
    if response.content and isinstance(response.content, str):
        converted_content = detect_and_convert_mermaid(response.content)
        if converted_content != response.content:
            # ìƒˆë¡œìš´ AIMessage ìƒì„± (contentê°€ ë³€ê²½ëœ ê²½ìš°)
            response = AIMessage(
                id=response.id,
                content=converted_content,
                tool_calls=response.tool_calls if hasattr(response, 'tool_calls') else [],
                additional_kwargs=response.additional_kwargs,
            )
    elif response.content and isinstance(response.content, list):
        # contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ë©€í‹°ëª¨ë‹¬ ë“±)
        new_content = []
        content_changed = False
        for item in response.content:
            if isinstance(item, str):
                converted = detect_and_convert_mermaid(item)
                if converted != item:
                    content_changed = True
                new_content.append(converted)
            elif isinstance(item, dict) and 'text' in item:
                converted = detect_and_convert_mermaid(item['text'])
                if converted != item['text']:
                    content_changed = True
                new_content.append({**item, 'text': converted})
            else:
                new_content.append(item)

        if content_changed:
            response = AIMessage(
                id=response.id,
                content=new_content,
                tool_calls=response.tool_calls if hasattr(response, 'tool_calls') else [],
                additional_kwargs=response.additional_kwargs,
            )

    # LLM ì‘ë‹µ ìºì‹± (íˆ´ í˜¸ì¶œì´ ì—†ëŠ” ìµœì¢… ì‘ë‹µë§Œ)
    if cache_key_content and not response.tool_calls:
        cache_data = {
            "content": response.content,
            "additional_kwargs": response.additional_kwargs,
            "id": response.id
        }
        cache_manager.set("llm", cache_key_content, cache_data)

    # Return the model's response as a list to be added to existing messages
    # ðŸ”¥ ëŒ€í™” ë§¥ë½ë„ í•¨ê»˜ ì—…ë°ì´íŠ¸
    return {
        "messages": [response],
        "conversation_context": conversation_context
    }


async def call_tools(state: State) -> Dict[str, List[ToolMessage]]:
    """ë™ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ë¡œë“œí•˜ê³  í˜¸ì¶œ"""
    # MCP ë„êµ¬ë¥¼ í¬í•¨í•œ ì „ì²´ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    all_tools = await get_all_tools()

    # ToolNodeì™€ ë™ì¼í•˜ê²Œ ë™ìž‘
    tool_node = ToolNode(all_tools)
    return await tool_node.ainvoke(state)


# Define a new graph

builder = StateGraph(State, input=InputState, config_schema=Configuration)

# Define the two nodes we will cycle between
builder.add_node(call_model)
builder.add_node("tools", call_tools)  # ë™ì  ë„êµ¬ ë¡œë“œ

# Set the entrypoint as `call_model`
# This means that this node is the first one called
builder.add_edge("__start__", "call_model")


def route_model_output(state: State) -> Literal["__end__", "tools"]:
    """Determine the next node based on the model's output.

    This function checks if the model's last message contains tool calls.

    Args:
        state (State): The current state of the conversation.

    Returns:
        str: The name of the next node to call ("__end__" or "tools").
    """
    last_message = state.messages[-1]
    if not isinstance(last_message, AIMessage):
        raise ValueError(
            f"Expected AIMessage in output edges, but got {type(last_message).__name__}"
        )
    # If there is no tool call, then we finish
    if not last_message.tool_calls:
        return "__end__"
    # Otherwise we execute the requested actions
    return "tools"


# Add a conditional edge to determine the next step after `call_model`
builder.add_conditional_edges(
    "call_model",
    # After call_model finishes running, the next node(s) are scheduled
    # based on the output from route_model_output
    route_model_output,
)

# Add a normal edge from `tools` to `call_model`
# This creates a cycle: after using tools, we always return to the model
builder.add_edge("tools", "call_model")

# Compile the builder into an executable graph with checkpointer
# MemorySaver allows the graph to store and retrieve conversation history
checkpointer = MemorySaver()
graph = builder.compile(name="ReAct Agent", checkpointer=checkpointer)
