"""Utility & helper functions."""

import re
import zlib
import base64
import logging
from typing import Tuple, Optional, List, Dict, Any
from collections import Counter

from langchain.chat_models import init_chat_model
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

logger = logging.getLogger(__name__)


def get_message_text(msg: BaseMessage) -> str:
    """Get the text content of a message."""
    content = msg.content
    if isinstance(content, str):
        return content
    elif isinstance(content, dict):
        return content.get("text", "")
    else:
        txts = [c if isinstance(c, str) else (c.get("text") or "") for c in content]
        return "".join(txts).strip()


def load_chat_model(fully_specified_name: str) -> BaseChatModel:
    """Load a chat model from a fully specified name.

    Args:
        fully_specified_name (str): String in the format 'provider/model'.
    """
    provider, model = fully_specified_name.split("/", maxsplit=1)
    return init_chat_model(model, model_provider=provider)


def mermaid_to_image_url(mermaid_code: str, output_format: str = "svg") -> str:
    """Mermaid ì½”ë“œë¥¼ kroki.io APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    kroki.ioëŠ” ë¬´ë£Œ ë‹¤ì´ì–´ê·¸ë¨ ë Œë”ë§ ì„œë¹„ìŠ¤ë¡œ, ì„œë²„ ì„¤ì¹˜ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

    Args:
        mermaid_code: Mermaid ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ
        output_format: ì¶œë ¥ í˜•ì‹ ("svg", "png", "pdf")

    Returns:
        ì´ë¯¸ì§€ URL (kroki.io í˜•ì‹)
    """
    # mermaid ì½”ë“œ ì •ë¦¬ (ì•ë’¤ ê³µë°± ì œê±°)
    mermaid_code = mermaid_code.strip()

    # zlibìœ¼ë¡œ ì••ì¶• í›„ base64 ì¸ì½”ë”© (URL-safe)
    compressed = zlib.compress(mermaid_code.encode('utf-8'), level=9)
    encoded = base64.urlsafe_b64encode(compressed).decode('ascii')

    # kroki.io URL ìƒì„±
    url = f"https://kroki.io/mermaid/{output_format}/{encoded}"

    logger.debug(f"[Mermaid] Generated kroki.io URL: {url[:100]}...")
    return url


def extract_mermaid_blocks(content: str) -> list[Tuple[str, str, int, int]]:
    """í…ìŠ¤íŠ¸ì—ì„œ mermaid ì½”ë“œ ë¸”ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        content: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸

    Returns:
        [(full_match, mermaid_code, start_pos, end_pos), ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
    """
    # ```mermaid ... ``` íŒ¨í„´ ë§¤ì¹­
    pattern = r'```mermaid\s*\n(.*?)```'
    matches = []

    for match in re.finditer(pattern, content, re.DOTALL | re.IGNORECASE):
        full_match = match.group(0)
        mermaid_code = match.group(1).strip()
        start_pos = match.start()
        end_pos = match.end()
        matches.append((full_match, mermaid_code, start_pos, end_pos))

    return matches


def process_mermaid_in_content(content: str, output_format: str = "svg") -> Tuple[str, bool]:
    """í…ìŠ¤íŠ¸ ë‚´ì˜ ëª¨ë“  mermaid ì½”ë“œ ë¸”ë¡ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    ```mermaid ... ``` í˜•ì‹ì˜ ì½”ë“œ ë¸”ë¡ì„ ì°¾ì•„ì„œ
    ![Mermaid Diagram](kroki_url) í˜•ì‹ì˜ ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        content: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
        output_format: ì´ë¯¸ì§€ ì¶œë ¥ í˜•ì‹ ("svg", "png")

    Returns:
        (ë³€í™˜ëœ í…ìŠ¤íŠ¸, ë³€í™˜ ì—¬ë¶€)
    """
    if not content or not isinstance(content, str):
        return content, False

    mermaid_blocks = extract_mermaid_blocks(content)

    if not mermaid_blocks:
        return content, False

    logger.info(f"[Mermaid] Found {len(mermaid_blocks)} mermaid block(s) to convert")

    # ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìœ„ì¹˜ê°€ ë³€ê²½ë˜ì§€ ì•Šë„ë¡ í•¨
    result = content
    for full_match, mermaid_code, start_pos, end_pos in reversed(mermaid_blocks):
        try:
            # mermaid ì½”ë“œë¥¼ ì´ë¯¸ì§€ URLë¡œ ë³€í™˜
            image_url = mermaid_to_image_url(mermaid_code, output_format)

            # ë‹¤ì´ì–´ê·¸ë¨ íƒ€ì… ì¶”ì¶œ (flowchart, sequenceDiagram, etc.)
            diagram_type = "Diagram"
            first_line = mermaid_code.split('\n')[0].strip().lower()
            if 'flowchart' in first_line or 'graph' in first_line:
                diagram_type = "í”Œë¡œìš°ì°¨íŠ¸"
            elif 'sequencediagram' in first_line or 'sequence' in first_line:
                diagram_type = "ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨"
            elif 'classDiagram' in first_line or 'class' in first_line:
                diagram_type = "í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨"
            elif 'gantt' in first_line:
                diagram_type = "ê°„íŠ¸ ì°¨íŠ¸"
            elif 'pie' in first_line:
                diagram_type = "íŒŒì´ ì°¨íŠ¸"
            elif 'erdiagram' in first_line or 'er' in first_line:
                diagram_type = "ER ë‹¤ì´ì–´ê·¸ë¨"
            elif 'statediagram' in first_line or 'state' in first_line:
                diagram_type = "ìƒíƒœ ë‹¤ì´ì–´ê·¸ë¨"
            elif 'journey' in first_line:
                diagram_type = "ì‚¬ìš©ì ì—¬ì •"
            elif 'timeline' in first_line:
                diagram_type = "íƒ€ì„ë¼ì¸"

            # ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            markdown_image = f"![{diagram_type}]({image_url})"

            # ì›ë³¸ mermaid ë¸”ë¡ì„ ì´ë¯¸ì§€ë¡œ êµì²´
            result = result[:start_pos] + markdown_image + result[end_pos:]

            logger.info(f"[Mermaid] Successfully converted {diagram_type}")

        except Exception as e:
            logger.error(f"[Mermaid] Failed to convert mermaid block: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
            continue

    return result, True


def detect_and_convert_mermaid(content: str) -> str:
    """ë©”ì‹œì§€ ë‚´ìš©ì—ì„œ mermaidë¥¼ ê°ì§€í•˜ê³  ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” í¸ì˜ í•¨ìˆ˜.

    Args:
        content: ì²˜ë¦¬í•  ë©”ì‹œì§€ ë‚´ìš©

    Returns:
        ë³€í™˜ëœ ë©”ì‹œì§€ ë‚´ìš© (mermaidê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜)
    """
    processed_content, was_converted = process_mermaid_in_content(content)

    if was_converted:
        logger.info("[Mermaid] Content was processed and mermaid blocks were converted to images")

    return processed_content


# =============================================================================
# ğŸ”¥ ëŒ€í™” ë§¥ë½ ë¶„ì„ (Conversation Context Analysis)
# =============================================================================

def extract_keywords_simple(text: str, top_n: int = 5) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬ ìœ„ì£¼)

    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜

    Returns:
        ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    # ë¶ˆìš©ì–´ (ì¡°ì‚¬, ì ‘ì†ì‚¬ ë“±)
    stopwords = {
        'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ë¡œ', 'ì™€', 'ê³¼',
        'ë„', 'ë§Œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì•Šë‹¤', 'ê°™ë‹¤', 'ìœ„í•´', 'ëŒ€í•œ',
        'í†µí•´', 'ë”°ë¼', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””',
        'ê·¸', 'ì €', 'ì´ëŸ°', 'ì €ëŸ°', 'ê²ƒ', 'ê±°', 'ë“±', 'ë°'
    }

    # í•œê¸€ ë‹¨ì–´ë§Œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
    words = re.findall(r'[ê°€-í£]{2,}', text.lower())

    # ë¶ˆìš©ì–´ ì œê±° ë° ë¹ˆë„ ê³„ì‚°
    filtered_words = [w for w in words if w not in stopwords]
    word_counts = Counter(filtered_words)

    # ìƒìœ„ Nê°œ ë°˜í™˜
    return [word for word, _ in word_counts.most_common(top_n)]


def detect_user_emotion(text: str) -> str:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê°ì • ê°ì§€

    Args:
        text: ì‚¬ìš©ì ë©”ì‹œì§€

    Returns:
        ê°ì§€ëœ ê°ì • ("neutral", "urgent", "curious", "frustrated")
    """
    text_lower = text.lower()

    # ê¸´ê¸‰/ë¶ˆì•ˆ
    if any(kw in text_lower for kw in ['ë¹¨ë¦¬', 'ê¸‰í•´', 'ë¬¸ì œ', 'ì•ˆë¼', 'ì˜¤ë¥˜', 'ì—ëŸ¬']):
        return "urgent"

    # ë¶ˆë§Œ/ì¢Œì ˆ
    if any(kw in text_lower for kw in ['ì´í•´ ì•ˆ', 'ëª¨ë¥´ê² ', 'ì–´ë ¤ì›Œ', 'ì‹¤íŒ¨', 'ì•ˆë˜ëŠ”']):
        return "frustrated"

    # í˜¸ê¸°ì‹¬/í•™ìŠµ
    if any(kw in text_lower for kw in ['ê¶ê¸ˆ', 'ì•Œê³  ì‹¶', 'ë°°ìš°', 'ì–´ë–»ê²Œ', 'ì™œ']):
        return "curious"

    return "neutral"


def detect_response_style(text: str) -> str:
    """ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” ì‘ë‹µ ìŠ¤íƒ€ì¼ ê°ì§€

    Args:
        text: ì‚¬ìš©ì ë©”ì‹œì§€

    Returns:
        ì‘ë‹µ ìŠ¤íƒ€ì¼ ("brief", "detailed", "practical")
    """
    text_lower = text.lower()

    # ê°„ë‹¨í•œ ë‹µë³€ ì„ í˜¸
    if any(kw in text_lower for kw in ['ê°„ë‹¨íˆ', 'ìš”ì•½', 'í•µì‹¬ë§Œ', 'ì§§ê²Œ']):
        return "brief"

    # ì‹¤ë¬´ ì¤‘ì‹¬
    if any(kw in text_lower for kw in ['ì‹¤ì œë¡œ', 'ì ìš©', 'ì—…ë¬´', 'ì‹¤ë¬´', 'ì–´ë–»ê²Œ í•˜']):
        return "practical"

    # ìƒì„¸ ì„¤ëª… ì„ í˜¸
    if any(kw in text_lower for kw in ['ìì„¸íˆ', 'ìƒì„¸íˆ', 'êµ¬ì²´ì ', 'ì™œ', 'ì´ìœ ']):
        return "detailed"

    return "detailed"  # ê¸°ë³¸ê°’


def extract_entities(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì—”í‹°í‹° ì¶”ì¶œ (íšŒì‚¬ëª…, ì œí’ˆëª…, ìˆ«ì ë“±)

    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸

    Returns:
        ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
    """
    entities = []

    # íšŒì‚¬ëª… íŒ¨í„´ (ì˜ˆ: "â—‹â—‹ì£¼ì‹íšŒì‚¬", "â—‹â—‹(ì£¼)")
    company_pattern = r'[ê°€-í£]{2,}(?:ì£¼ì‹íšŒì‚¬|\(ì£¼\)|ê¸°ì—…|íšŒì‚¬)'
    entities.extend(re.findall(company_pattern, text))

    # ë°°ì¶œê¶Œ ê´€ë ¨ ì½”ë“œ (ì˜ˆ: "KOC", "KCU", "KAU")
    emission_codes = re.findall(r'\b[A-Z]{3}\b', text)
    entities.extend(emission_codes)

    # ìˆ«ì + ë‹¨ìœ„ (ì˜ˆ: "1000í†¤", "500ë§Œì›")
    number_patterns = re.findall(r'\d+(?:,\d{3})*(?:í†¤|ì›|ê°œ|ê±´|ë…„|ì›”|ì¼)', text)
    entities.extend(number_patterns)

    return list(set(entities))  # ì¤‘ë³µ ì œê±°


def determine_conversation_stage(messages: List[BaseMessage]) -> str:
    """ëŒ€í™” ë‹¨ê³„ íŒë‹¨

    Args:
        messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸

    Returns:
        ëŒ€í™” ë‹¨ê³„ ("initial", "progressing", "advanced")
    """
    # ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì¹´ìš´íŠ¸ (HumanMessage)
    user_message_count = sum(1 for m in messages if isinstance(m, HumanMessage))

    if user_message_count <= 2:
        return "initial"
    elif user_message_count <= 5:
        return "progressing"
    else:
        return "advanced"


def analyze_conversation_context(messages: List[BaseMessage]) -> Dict[str, Any]:
    """ëŒ€í™” ì´ë ¥ì„ ë¶„ì„í•˜ì—¬ ë§¥ë½ ì •ë³´ ì¶”ì¶œ

    Args:
        messages: ì „ì²´ ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸

    Returns:
        ëŒ€í™” ë§¥ë½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        {
            "recent_topics": List[str],  # ìµœê·¼ 3ê°œ ì£¼ì œ(í‚¤ì›Œë“œ)
            "user_emotion": str,  # ì‚¬ìš©ì ê°ì •
            "response_style": str,  # ì„ í˜¸ ì‘ë‹µ ìŠ¤íƒ€ì¼
            "mentioned_entities": List[str],  # ì–¸ê¸‰ëœ ì—”í‹°í‹°
            "conversation_stage": str,  # ëŒ€í™” ë‹¨ê³„
            "question_count": int,  # ì´ ì§ˆë¬¸ ìˆ˜
        }
    """
    context = {
        "recent_topics": [],
        "user_emotion": "neutral",
        "response_style": "detailed",
        "mentioned_entities": [],
        "conversation_stage": "initial",
        "question_count": 0,
    }

    # ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
    if not messages or len(messages) < 2:
        return context

    # ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì¶”ì¶œ
    user_messages = [
        get_message_text(m)
        for m in messages
        if isinstance(m, HumanMessage)
    ]

    if not user_messages:
        return context

    context["question_count"] = len(user_messages)

    # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ ë¶„ì„
    recent_messages = user_messages[-3:]

    # 1. ìµœê·¼ ì£¼ì œ(í‚¤ì›Œë“œ) ì¶”ì¶œ
    all_recent_text = " ".join(recent_messages)
    context["recent_topics"] = extract_keywords_simple(all_recent_text, top_n=5)

    # 2. ìµœê·¼ ë©”ì‹œì§€ì—ì„œ ê°ì • ê°ì§€
    if recent_messages:
        context["user_emotion"] = detect_user_emotion(recent_messages[-1])

    # 3. ì‘ë‹µ ìŠ¤íƒ€ì¼ ê°ì§€
    if recent_messages:
        context["response_style"] = detect_response_style(recent_messages[-1])

    # 4. ì „ì²´ ëŒ€í™”ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
    all_text = " ".join(user_messages)
    context["mentioned_entities"] = extract_entities(all_text)

    # 5. ëŒ€í™” ë‹¨ê³„ íŒë‹¨
    context["conversation_stage"] = determine_conversation_stage(messages)

    logger.info(f"[Context] ë¶„ì„ ì™„ë£Œ: topics={context['recent_topics']}, "
                f"emotion={context['user_emotion']}, stage={context['conversation_stage']}")

    return context


def build_context_aware_prompt_addition(context: Dict[str, Any]) -> str:
    """ëŒ€í™” ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ ì„¹ì…˜ ìƒì„±

    Args:
        context: analyze_conversation_context()ì˜ ë°˜í™˜ê°’

    Returns:
        í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•  ë§¥ë½ ì •ë³´ ë¬¸ìì—´
    """
    if not context or context.get("question_count", 0) <= 1:
        return ""  # ì²« ëŒ€í™”ëŠ” ë§¥ë½ ì—†ìŒ

    sections = []

    # ëŒ€í™” ë‹¨ê³„
    stage_map = {
        "initial": "ì´ˆê¸° ëŒ€í™” ë‹¨ê³„ - ê¸°ë³¸ ê°œë… ìœ„ì£¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”",
        "progressing": "ëŒ€í™” ì§„í–‰ ì¤‘ - ì‚¬ìš©ìê°€ ì´ë¯¸ ê¸°ë³¸ì„ ì´í•´í–ˆë‹¤ê³  ê°€ì •í•˜ì„¸ìš”",
        "advanced": "ì‹¬í™” ëŒ€í™” ë‹¨ê³„ - ì „ë¬¸ì ì´ê³  êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì œê³µí•˜ì„¸ìš”"
    }
    stage = context.get("conversation_stage", "initial")
    sections.append(f"**ëŒ€í™” ë‹¨ê³„**: {stage_map.get(stage, '')}")

    # ìµœê·¼ ì£¼ì œ
    topics = context.get("recent_topics", [])
    if topics:
        topics_str = ", ".join(topics[:3])
        sections.append(f"**ìµœê·¼ ë…¼ì˜ëœ ì£¼ì œ**: {topics_str}")
        sections.append(f"â†’ ì´ ì£¼ì œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì—°ì†ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”")

    # ì‚¬ìš©ì ê°ì •
    emotion = context.get("user_emotion", "neutral")
    emotion_guide = {
        "urgent": "âš¡ ì‚¬ìš©ìê°€ ê¸‰í•œ ìƒí™©ì…ë‹ˆë‹¤. í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì œê³µí•˜ê³ , ë‹¨ê³„ë³„ ì•¡ì…˜ì„ ëª…í™•íˆ í•˜ì„¸ìš”.",
        "frustrated": "ğŸ˜“ ì‚¬ìš©ìê°€ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ë” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ê³ , ë‹¨ê³„ë¥¼ ì„¸ë¶„í™”í•˜ì„¸ìš”.",
        "curious": "ğŸ¤” ì‚¬ìš©ìê°€ í•™ìŠµ ì˜ìš•ì´ ë†’ìŠµë‹ˆë‹¤. ìƒì„¸í•˜ê³  êµìœ¡ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        "neutral": ""
    }
    if emotion != "neutral":
        sections.append(f"**ì‚¬ìš©ì ìƒíƒœ**: {emotion_guide.get(emotion, '')}")

    # ì‘ë‹µ ìŠ¤íƒ€ì¼
    style = context.get("response_style", "detailed")
    style_guide = {
        "brief": "ğŸ“ ê°„ê²°í•œ ë‹µë³€ì„ ì›í•©ë‹ˆë‹¤. í•µì‹¬ë§Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ì œê³µí•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ ì„ íƒì ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”.",
        "practical": "ğŸ’¼ ì‹¤ë¬´ ì ìš©ì„ ì›í•©ë‹ˆë‹¤. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ì˜ˆì‹œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
        "detailed": "ğŸ“š ìƒì„¸í•œ ì„¤ëª…ì„ ì›í•©ë‹ˆë‹¤. ë°°ê²½, ì´ìœ , ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì—¬ ì¶©ë¶„íˆ ì„¤ëª…í•˜ì„¸ìš”."
    }
    sections.append(f"**ë‹µë³€ ìŠ¤íƒ€ì¼**: {style_guide.get(style, '')}")

    # ì–¸ê¸‰ëœ ì—”í‹°í‹°
    entities = context.get("mentioned_entities", [])
    if entities:
        entities_str = ", ".join(entities[:5])
        sections.append(f"**ëŒ€í™” ì¤‘ ì–¸ê¸‰ëœ ë‚´ìš©**: {entities_str}")
        sections.append(f"â†’ ì´ì „ì— ì–¸ê¸‰ëœ ë‚´ìš©ì€ ë‹¤ì‹œ ì„¤ëª…í•˜ì§€ ë§ê³  ì°¸ì¡°ë§Œ í•˜ì„¸ìš”")

    # ìµœì¢… ì¡°í•©
    prompt_addition = "\n\n" + "=" * 60 + "\n"
    prompt_addition += "ğŸ“ **ëŒ€í™” ë§¥ë½ ì •ë³´ (ì´ì „ ëŒ€í™” ê¸°ë°˜)**\n"
    prompt_addition += "=" * 60 + "\n"
    prompt_addition += "\n".join(sections)
    prompt_addition += "\n" + "=" * 60 + "\n"

    return prompt_addition
